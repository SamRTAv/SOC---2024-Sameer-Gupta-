# -*- coding: utf-8 -*-
"""Copy of Assignment_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YFEYqd2cRpdHO_zhr8_oocbLP7I5QYvv
"""

import numpy as np
import matplotlib.pyplot as plt
import math
import random

"""## Linear Regression"""

# # define an array x of atleast 100 points with a separation of at least 1
x = []
while len(x) < 100:
  b = random.randint(1,200)
  if b not in x:
    x.append(b)

# print(x)





# # now choose any 2 values for A, B and generate an array y_true such that y_true = Ax + B for all x defined above
A = 3
B = -5
y_true = []
for num in x:
  y_true.append(A*num +B)

# # Add Gaussian Noise which can vary from upto -20 to 20 to y_true to get our data
data = []
for num in y_true:
  data.append(num - random.randint(-20,20))

# print the data, y_true and x
y_true = np.array(y_true)
x = np.array(x)
data = np.array(data)
print((x))
print((y_true))
print((data))
# print(data.shape)
# print(x.shape)

# plot scatterplot between data and x
# also plot y_true vs x on the same plot
plt.figure()
plt.scatter(x, data)
plt.xlabel('x')
plt.ylabel('data')
# plt.figure()
plt.scatter(x, y_true)
plt.xlabel('x')
plt.ylabel('data')

# # write a function definition to calculate loss of linear Regression
def loss_function(data, y_true):
  squared_differences = np.square(data - y_true)
  return np.mean(squared_differences)

print(loss_function(data,y_true))

# write a function definition to calculate derivative of cost function of linear regression with respect to parameter A

def derivative_wrt_a(x, data, a, b):
  return 2 * np.mean(x * (data - (a * x + b)))

# write a function definition to calculate derivative of cost function of linear regression with respect to parameter B
def derivative_wrt_b(x, data, a, b):
  return 2 * np.mean(data - (a * x + b))

# define A = 0 and B = 0 randomly
# also define 2 lists to store values of A and B at various iterations
# A= 0
# B = 0
# # listA  = []
# # listB = []

# compute the model array based on initial values of A and B
# Also print the cost function for the data before training
print(loss_function(data,y_true))

# Define the Learning rate, alpha randomly. You will have to modify it during training

alpha = 0.000001
#for keeping iterations near 2000 alpha cannot be greater than 0.000001 otherwise large errors are intrduced

# write the main training loop and train for atleast 2000 iterations
# print the cost at each iteration while training
# save the values of A and B at atleast 5 different iterations spaced by atleast 50 into the lists defined earlier
# Also, make sure you use the same model to find derivative of cost wrto A and B and then modify A and B together
listA, listB, cost = [], [], []
for i in range(1000):
  loss = loss_function(data, A * x + B)
  cost.append(loss)
  dJA = derivative_wrt_a(x, data, A, B)
  dJB = derivative_wrt_b(x, data, A, B)
  A -= alpha * dJA
  B -= alpha * dJB
  # print(A)
  if (i + 1) % 50 == 0:
    listA.append(A)
    listB.append(B)

print(cost)
# print(A)
# print(B)

"""Comment how many iterations were sufficient to arrive near the actual solution (Let's hope it's less than 2000).

Use Multiple values of A and B in the initial step of generating y_true.
"""

# print the two lists for A and B and see how those values change with number of iterations
print(listA)
print(listB)

# generate different models for A and B you stored in the lists while training
mod1 = []
for i in range(len(x)):
  mod1.append(listA[0]*x[i]+ listB[0])
mod2 = []
for i in range(len(x)):
  mod2.append(listA[1]*x[i]+ listB[1])
mod3 = []
for i in range(len(x)):
  mod3.append(listA[2]*x[i]+ listB[2])
mod4 = []
for i in range(len(x)):
  mod4.append(listA[3]*x[i]+ listB[3])

# plot the different models you defined earlier and also y_true vs x on the same plot
# use different colours, though the lines may not be visible distinctly which is ok
plt.plot(x, mod1, label="mod1", color="blue")
plt.plot(x, mod2, label="mod2", color="black")
plt.plot(x, mod3, label="mod3", color="yellow")
plt.scatter(x, data, color = "green")
plt.xlabel('x')
plt.ylabel('data')
plt.plot(x, y_true, label="y_true", color = "orange")
plt.xlabel('x')
plt.ylabel('y_true')
plt.legend()
plt.show()

"""The key takeaway is that by carefully selecting the learning rate (alpha), you can get very close to the actual solution in linear regression with only a few iterations. This is crucial because you should always aim to use the least amount of computational resources when training a model.

In our example, we used just one variable (x) to make it easier to visualize the training process. However, the same approach applies to linear regression with multiple variables.

## Logistic Regression

Using the following data, let's train a Logistic Regression model. I have already plotted the data for visualization, noting that I intentionally included some overlapping examples to reflect real-world scenarios where a clear distinguishing line isn't always present.
"""

# so x1 and x2 are two different parameters on which aur  prediction function will depend
# on the onter hand x2_1 indicates all the values of parameter x2 on which our prediction function is 1 and same goes for everyother term


x2_1 = np.random.randint(30, 70, 50)
x1_1 = np.random.randint(15, 50, 50)
x1_0 = np.random.randint(45, 90, 50)
x2_0 = np.random.randint(5, 45, 50)

# print(x2_1)
# print(x2_0)
# we have to make a single vector for each of x1 and x2 by combining both the classes for training
x1 = np.hstack((x1_0, x1_1))
x2 = np.hstack((x2_0, x2_1))
# print(x2)
# dividing both x1 and x2 by large number like 1000 to make sure that their sigmoid values don't saturate to either 0 or 1
x1 = x1/1000
x2 = x2/1000
# creating data
y0 = np.zeros(50)
y1 = np.ones(50)
data = np.hstack((y0, y1))

plt.scatter(x1_1, x2_1, color="blue")
plt.scatter(x1_0, x2_0, color="red")
plt.show()

"""## Line we are considering as a distinguishing line is-

$$
θ_0 +θ_1*x_1 +θ_2*x_2 = 0
$$
"""

# sigmoid function
def sigmoid(z):
  return 1/(1+ np.exp(-(z)))

# where z will be (a*x1+b*x2 + c)

# function to compute cost of Logistic Regression
def likelihood(data,hx):
  return np.mean(data* np.log(hx) + (1-data) * np.log(1-hx))

# function to compute the derivative of cost with respect to theta1
def derivative_wrt_a(x1,data,hx):
  return np.mean((data - hx)*x1)

# function to compute the derivative of cost with respect to theta2

def derivative_wrt_b(x2,data,hx):
  return np.mean((data - hx)*x2)

# function to compute the derivative of cost with respect to theta0

def derivative_wrt_c(data,hx):
  return np.mean((data - hx))

# randomly initialize theta0, theta1, theta2 and compute and print the initial cost
a = 2
b = 3
c = 1
data = np.array(data)
x1 = np.array(x1)
x2 = np.array(x2)
z = a * x1 + b * x2 + c
hx = sigmoid(z)
# print(hx)
# print(np.log(z))
# print(data)
# z = np.array(z)
print(likelihood(data,hx))

# learning rate, alpha. You will have to modify it while training
alpha = 0.001

# main training loop
# Make sure you use the same model to find derivative of cost wrto theta0, theta1 and theta2 and then modify them together
listA, listB, listC, cost = [], [], [],[]
for i in range(500):
  z = a * x1 + b * x2 + c
  hx = sigmoid(z)
  loss = likelihood(data, hx)
  cost.append(loss)
  dJA = derivative_wrt_a(x1, data, hx)
  dJB = derivative_wrt_b(x2, data, hx)
  dJC = derivative_wrt_c(data, hx)
  a += alpha * dJA
  b += alpha * dJB
  c += alpha * dJC
  # print(A)
  if (i + 1) % 50 == 0:
    listA.append(a)
    listB.append(b)
    listC.append(c)

print(cost)
print(listA)
print(listB)
print(listC)
# print(B)

"""Modify the values of theta_1 and theta0 for ploting;

Read below:

Earlier, we scaled down both `x_1` and `x_2` by 1000 for easier training. Now, we need to multiply both sides by 1000.

So,

$$
1000x_2 = \left(\frac{-1000 \theta_1}{\theta_2}\right) x_1 + \left(\frac{-1000 \theta_0}{\theta_2}\right)
$$

This simplifies to:

$$
y = \theta_{1,\text{new}} x_1 + \theta_{0,\text{new}}
$$

Finally, plot `y` vs `1000x_1` using the plot function.

"""

# plt.scatter(x, data, color = "green")
# plt.xlabel('x')
# plt.ylabel('data')
# plt.scatter(1000*x2, 1000*x1, color = "orange")
# plt.xlabel('x')
# plt.ylabel('y_true')
# plt.legend()
# plt.show()

# plot the final line obtained after training along with the initial data on the same plot
